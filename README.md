# Query, Localize, Explain: A Language-Guided Framework for Open-Set Video Event Localization

[![Python 3.9](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-blue)](https://github.com/huggingface/transformers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Paper Status](https://img.shields.io/badge/paper-in%20progress-brightgreen)](./)

---

An explainable, language-guided framework for open-set temporal action localization (TAL) on endoscopic videos.
This repository contains the official implementation accompanying the dissertation â€œAn Explainable, Language-Guided Framework for Open-Set Temporal Localization on Endoscopic Videosâ€ by Soheil Jafarifard Bidgoli (MSc Computer Science, Aston University).

ğŸš€ Overview

Surgical and endoscopic procedures generate long, complex videos where clinicians may wish to localize arbitrary events described in natural language (e.g., â€œwhen the clipper is appliedâ€).
Traditional methods are closed-set: they only recognize fixed phases or tools. Our framework goes further:

Open-vocabulary localization â€“ Query with free-form clinical language, not just predefined labels.

Efficient temporal modeling â€“ Supports long untrimmed videos via Transformer and State-Space (Mamba) backbones.

Explainability & trustworthiness â€“ Produces attention heatmaps and uncertainty estimates for clinical safety.

ğŸ—ï¸ Framework Architecture

The system is a multi-stage pipeline combining computer vision, natural language processing, and temporal reasoning:

Vision Backbone

Default: MÂ²CRL pretrained video transformer for robust spatio-temporal features.

Alternative: EndoMamba backbone (structured state-space model) for efficient long-video scaling.

Text Encoder

CLIP-based transformer encoder.

Fine-tuned with LoRA (parameter-efficient tuning) for medical terminology.

Language-Guided Fusion Head

Cross-modal transformer aligns video features with text queries.

Outputs frame-level relevance scores + attention heatmaps.

Temporal Head

Baseline: Lightweight Transformer for sequence modeling.

SOTA Upgrade: Mamba-based SSM for linear-complexity long-sequence analysis.

Uncertainty & Explainability

Evidential Deep Learning (EDL) adds calibrated uncertainty scores.

Cross-attention maps provide visual explanations.

Language-Guided-Endoscopy-Localization/
â”‚
â”œâ”€â”€ backbone/                         # Vision backbones
â”‚   â”œâ”€â”€ endomamba.py                  # EndoMamba (SSM-based backbone)
â”‚   â””â”€â”€ vision_transformer.py         # ViT-based backbone (MÂ²CRL, etc.)
â”‚
â”œâ”€â”€ checkpoints/                      # Saved checkpoints and logs
â”‚
â”œâ”€â”€ comparison_models/                # Baseline and benchmark models
â”‚   â”œâ”€â”€ clip_baseline/
â”‚   â”‚   â””â”€â”€ clip_baseline.py          # CLIP zero-shot / linear probe
â”‚   â”‚
â”‚   â”œâ”€â”€ Moment-DETR/                  # Moment-DETR temporal grounding
â”‚   â”‚   â”œâ”€â”€ run_evaluation.py
â”‚   â”‚   â”œâ”€â”€ run_feature_extraction.py
â”‚   â”‚   â”œâ”€â”€ run_preprocessing.py
â”‚   â”‚   â”œâ”€â”€ run_training.py
â”‚   â”‚   â””â”€â”€ moment_detr_module/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ configs.py
â”‚   â”‚       â”œâ”€â”€ dataset.py
â”‚   â”‚       â”œâ”€â”€ engine.py
â”‚   â”‚       â”œâ”€â”€ loss.py
â”‚   â”‚       â”œâ”€â”€ matcher.py
â”‚   â”‚       â”œâ”€â”€ modeling.py
â”‚   â”‚       â”œâ”€â”€ position_encoding.py
â”‚   â”‚       â”œâ”€â”€ transformer.py
â”‚   â”‚       â”œâ”€â”€ utils.py
â”‚   â”‚       â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ xclip_baseline/               # X-CLIP video-language baseline
â”‚       â”œâ”€â”€ train_xclip.py
â”‚       â”œâ”€â”€ eval_xclip.py
â”‚       â”œâ”€â”€ infer_xclip.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ project_config.py
â”‚       â”œâ”€â”€ README_XCLIP.md
â”‚       â””â”€â”€ xclip_package/
â”‚           â””â”€â”€ xclip/
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ data.py
â”‚               â”œâ”€â”€ losses.py
â”‚               â”œâ”€â”€ metrics.py
â”‚               â”œâ”€â”€ model.py
â”‚               â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ dataset_preprocessing/            # Preprocessing for Cholec80 dataset
â”‚   â”œâ”€â”€ create_splits.py
â”‚   â”œâ”€â”€ extract_cholec80_frames.py
â”‚   â””â”€â”€ prepare_cholec80.py
â”‚
â”œâ”€â”€ pretrained/                       # Pretrained model weights
â”‚   â””â”€â”€ checkpoint.pth
â”‚
â”œâ”€â”€ dataset.py                        # Dataset wrapper
â”œâ”€â”€ inference.py                      # Inference script (language-guided)
â”œâ”€â”€ models.py                         # Main model components
â”œâ”€â”€ project_config.py                 # Config file for project settings
â”œâ”€â”€ train.py                          # Training entry point
â”‚
â”œâ”€â”€ README.md                         # Project documentation
â””â”€â”€ .gitignore

ğŸ“Š Baselines & Comparisons

We benchmark against three families of baselines
:

Visionâ€“Language Models (open-vocabulary)

CLIP (zero-shot & linear-probe)

VideoCLIP / X-CLIP

Temporal Grounding Models (language â†’ time interval)

Moment-DETR

Surgical Specialists (closed-set baselines)

TeCNO

ğŸ“ˆ Evaluation Protocol

Frame-level Relevance (main task): AUROC, AUPRC, mAP@frame

Temporal Grounding: mAP@tIoU=0.3/0.5/0.7, R@1/R@5

Phase & Tool Recognition: Accuracy, Macro-F1, mAP

Uncertainty calibration and explainability are also reported via EDL and attention maps.

ğŸ§‘â€âš•ï¸ Dataset

Cholec80 laparoscopic cholecystectomy dataset (80 full-length surgeries).

Preprocessing pipeline generates:

Frame sequences

Language queries (phases + tools)

Positive/negative triplets for training

âš™ï¸ Training

Frame Extraction

python datasets/extract_cholec80_frames.py --input data/raw --output data/frames


Prepare Metadata & Splits

python datasets/prepare_cholec80.py
python datasets/create_splits.py


Train Model

python training/train.py --config configs/m2crl_baseline.yaml

ğŸ”¬ Key Features

Bi-Level Consistency Loss: Aligns semantic + motion features using optical flow
.

Evidential Deep Learning: Outputs both predictions & uncertainty estimates.

Flexible Backbones: Swap between Transformer and Mamba-based models.

Parameter-Efficient Tuning: LoRA adapters minimize fine-tuning cost.

Cross-modal Explainability: Visualize attention maps for clinician review.

ğŸ“š Citation

If you use this framework in your research, please cite:

@mastersthesis{jafarifard2025,
  title={An Explainable, Language-Guided Framework for Open-Set Temporal Localization on Endoscopic Videos},
  author={Soheil Jafarifard Bidgoli},
  school={Aston University},
  year={2025}
}

ğŸ¤ Acknowledgements

Supervisor: Dr. Zhuangzhuang Dai

Aston University â€“ MSc Computer Science, CS4700 Dissertation

OpenAI CLIP, MÂ²CRL, VideoMamba, Moment-DETR, and the Cholec80 community

âœ¨ This repo aims to push forward trustworthy, explainable AI for surgery by combining vision, language, and temporal reasoning.

@article{jafari2025query,
  title={Query, Localize, Explain: A Language-Guided Framework for Open-Set Event Localization in Medical Video},
  author={Jafari, Soheil and Dr. Zhuangzhuang Dai},
  journal={arXiv preprint arXiv:25XX.XXXXX},
  year={2025}
}
